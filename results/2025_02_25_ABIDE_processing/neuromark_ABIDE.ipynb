{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88126d0-e0bd-47b6-afa8-74046447566d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86cb1e0c-47bf-4c6f-bc1c-4656b2c157f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Example Python script illustrating the multi-step ICA pipeline described in your excerpt.\n",
    "\n",
    "Required libraries:\n",
    "    numpy, scipy, scikit-learn, nibabel (for loading NIfTI, if needed), etc.\n",
    "    \n",
    "DISCLAIMER:\n",
    "    - This is a simplified illustration only. Adjust as needed for your environment.\n",
    "    - Actual Infomax ICA and true ICASSO procedures may differ in details from FastICA.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from scipy.stats import skew\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Step 1: Load and prepare data\n",
    "# ---------------------------------------------------------------------------\n",
    "def load_fmri_data(list_of_nifti_paths):\n",
    "    \"\"\"\n",
    "    Example loader that:\n",
    "      - Reads each preprocessed 4D fMRI file (time x x_dim x y_dim x z_dim).\n",
    "      - Reshapes to a 2D array: (time, n_voxels).\n",
    "      - Returns a list of subject data arrays.\n",
    "    \"\"\"\n",
    "    subject_data = []\n",
    "    for fpath in list_of_nifti_paths:\n",
    "        img = nib.load(fpath)\n",
    "        data_4d = img.get_fdata()  # shape: (x_dim, y_dim, z_dim, time)\n",
    "        # Move time to axis=0 and flatten the spatial dims:\n",
    "        data_2d = np.reshape(np.moveaxis(data_4d, -1, 0),\n",
    "                             (data_4d.shape[-1], -1))\n",
    "        subject_data.append(data_2d)\n",
    "    return subject_data\n",
    "\n",
    "\n",
    "def individual_subject_pca(subject_data, n_components=110):\n",
    "    \"\"\"\n",
    "    Perform PCA on each subject’s data (time x voxel) to reduce to `n_components`.\n",
    "    Return a list of reduced 2D arrays: (time, n_components).\n",
    "    \"\"\"\n",
    "    subject_pcs = []\n",
    "    for data_2d in subject_data:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        reduced = pca.fit_transform(data_2d)  # shape: (time_points, n_components)\n",
    "        subject_pcs.append(reduced)\n",
    "    return subject_pcs\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Step 2 & 3: Concatenate individual PCs and run group-level PCA, then ICA\n",
    "# ---------------------------------------------------------------------------\n",
    "def run_group_pca_then_ica(subject_pcs, n_group_components=100,\n",
    "                           n_ica_runs=100, random_state=0):\n",
    "    \"\"\"\n",
    "    - Concatenate each subject's PCA results along the row dimension (time),\n",
    "      forming a large group matrix: (sum_of_times, 110).\n",
    "    - Run a second PCA to reduce to `n_group_components` (default=100).\n",
    "    - Then run multiple ICA (FastICA) attempts for ICASSO-like approach.\n",
    "    - Pick best run based on similarity to a \"consensus\" or by max neg-entropy, etc.\n",
    "    - Return the best-run's IC mixing matrix and the final group-level ICs.\n",
    "    \"\"\"\n",
    "    # 1) Concatenate:\n",
    "    # subject_pcs is a list of arrays shape (time, 110)\n",
    "    group_data = np.concatenate(subject_pcs, axis=0)  # (sum_of_times, 110)\n",
    "    \n",
    "    # 2) PCA to reduce to 100 group-level PCs\n",
    "    group_pca = PCA(n_components=n_group_components, random_state=random_state)\n",
    "    group_pcs_data = group_pca.fit_transform(group_data)  # shape: (sum_of_times, 100)\n",
    "    \n",
    "    # For an \"ICASSO-like\" approach, run multiple ICA with different random seeds:\n",
    "    ica_components_list = []\n",
    "    \n",
    "    # Typically, you might store all unmixing matrices & then do a clustering step.\n",
    "    # Here we do a simplified approach, then pick the \"best\" by average kurtosis, e.g.\n",
    "    for run_idx in range(n_ica_runs):\n",
    "        rng = np.random.RandomState(run_idx)  # or vary seeds\n",
    "        ica_model = FastICA(n_components=n_group_components,\n",
    "                            random_state=rng,\n",
    "                            max_iter=1000,\n",
    "                            whiten=True)\n",
    "        S_ = ica_model.fit_transform(group_pcs_data)  # shape: (time_points, 100)\n",
    "        # The columns of `S_` are the estimated IC time-courses (component signals).\n",
    "        # The mixing matrix W^-1 is ica_model.mixing_. \n",
    "        # The actual spatial maps can be derived from S_ or from the pseudo-inverse as needed.\n",
    "        # We'll store the \"spatial\" IC patterns as S_.T or do a pinv if you prefer that orientation.\n",
    "        ica_components_list.append(S_.T)\n",
    "    \n",
    "    # Decide which run is \"best\" – e.g., pick run with highest average neg-entropy or kurtosis:\n",
    "    # (Below is a crude example using the sum of absolute kurtosis across all components.)\n",
    "    best_run_idx = None\n",
    "    best_run_metric = -np.inf\n",
    "    for i, comps_2d in enumerate(ica_components_list):\n",
    "        # `comps_2d` shape: (n_components, time_points)\n",
    "        # we could use kurtosis, or negentropy approximation, etc.\n",
    "        # for simplicity, we do:\n",
    "        kurt_vals = np.array([skew(c, bias=False) for c in comps_2d])\n",
    "        # sum of absolute skew:\n",
    "        metric = np.sum(np.abs(kurt_vals))\n",
    "        if metric > best_run_metric:\n",
    "            best_run_metric = metric\n",
    "            best_run_idx = i\n",
    "    \n",
    "    best_ica_maps = ica_components_list[best_run_idx]  # shape: (100, time_points)\n",
    "    \n",
    "    # Reshape or invert to get group-level \"spatial\" components. \n",
    "    # For Infomax-like ICA on PCA outputs, we typically interpret components \n",
    "    # by (pseudo-inverse of mixing) or the dot product with group PCA loadings.\n",
    "    # This example just returns best_ica_maps as \"group-level ICs\" to be refined if needed.\n",
    "    return best_ica_maps  # shape: (n_components, time_points)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Step 4: Flip IC if its skewness is negative\n",
    "# ---------------------------------------------------------------------------\n",
    "def flip_negative_skew(ics_2d):\n",
    "    \"\"\"\n",
    "    Given 2D array of shape (n_components, n_timepoints/voxels),\n",
    "    compute skewness for each row, flip if negative.\n",
    "    Returns the array with flips applied.\n",
    "    \"\"\"\n",
    "    flipped_ics = ics_2d.copy()\n",
    "    n_comp = flipped_ics.shape[0]\n",
    "    for i in range(n_comp):\n",
    "        # compute skewness for the ith row\n",
    "        val = skew(flipped_ics[i, :], bias=False)\n",
    "        if val < 0:\n",
    "            flipped_ics[i, :] *= -1\n",
    "    return flipped_ics\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Step 5: Greedy matching of two sets of IC maps\n",
    "# ---------------------------------------------------------------------------\n",
    "def greedy_spatial_match(ics_a, ics_b, corr_threshold=0.4):\n",
    "    \"\"\"\n",
    "    ics_a, ics_b: each shape = (n_components, n_voxels) or (n_components, n_timepoints)\n",
    "                  but typically you'd have them in \"spatial map\" form (component x voxel).\n",
    "    \n",
    "    1. Compute an abs-correlation matrix between the two sets of ics, shape= (Na, Nb).\n",
    "    2. Repeatedly pick the max correlation pair, sign-flip if original correlation is negative.\n",
    "    3. Zero out that row and column to exclude them from further pairing.\n",
    "    4. Return the matched pairs that exceed the threshold, plus their sign-flipped versions.\n",
    "    \"\"\"\n",
    "    nA, nV = ics_a.shape\n",
    "    nB, _ = ics_b.shape\n",
    "    # correlation matrix (absolute value)\n",
    "    # We'll keep track of the sign as well. \n",
    "    corr_mat = np.zeros((nA, nB))\n",
    "    sign_mat = np.zeros((nA, nB))\n",
    "    \n",
    "    for i in range(nA):\n",
    "        for j in range(nB):\n",
    "            corr_ij = np.corrcoef(ics_a[i,:], ics_b[j,:])[0,1]\n",
    "            corr_mat[i,j] = abs(corr_ij)\n",
    "            sign_mat[i,j] = np.sign(corr_ij)  # +1 or -1 or 0\n",
    "    \n",
    "    # Now do the iterative \"greedy\" selection:\n",
    "    matched_pairs = []  # will store tuples like (idxA, idxB, correlation, sign_of_correlation)\n",
    "    \n",
    "    # Make a copy of corr_mat to zero out as we pick pairs\n",
    "    tmp_corr = corr_mat.copy()\n",
    "    \n",
    "    for _ in range(nA):  # up to min(nA,nB) matches\n",
    "        # find max in tmp_corr\n",
    "        i_max, j_max = np.unravel_index(np.argmax(tmp_corr), tmp_corr.shape)\n",
    "        max_val = tmp_corr[i_max, j_max]\n",
    "        if max_val < corr_threshold:\n",
    "            # no more pairs exceed threshold\n",
    "            break\n",
    "        \n",
    "        matched_pairs.append((i_max, j_max, max_val, sign_mat[i_max,j_max]))\n",
    "        \n",
    "        # zero out that row and column\n",
    "        tmp_corr[i_max, :] = 0.0\n",
    "        tmp_corr[:, j_max] = 0.0\n",
    "    \n",
    "    # If correlation sign was negative, we sign-flip ICS_B’s component \n",
    "    # or ICS_A’s, but typically flip ICS_B for convenience. \n",
    "    # (In actual pipeline, you might want to store a separate copy for the flips.)\n",
    "    # We'll do it in-place here for demonstration.\n",
    "    for (ia, ib, val, sgn) in matched_pairs:\n",
    "        if sgn < 0:\n",
    "            ics_b[ib,:] *= -1\n",
    "    \n",
    "    # Return the subset of matched pairs that exceed threshold \n",
    "    # plus the possibly sign-flipped ics_b:\n",
    "    final_pairs = [p for p in matched_pairs if p[2] >= corr_threshold]\n",
    "    return final_pairs, ics_a, ics_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4455099b-cc98-4b8e-8369-45d1e0183682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Main demonstration function\n",
    "# ---------------------------------------------------------------------------\n",
    "def main_example():\n",
    "    \"\"\"\n",
    "    Demonstrate the pipeline, step by step, using hypothetical file lists\n",
    "    for two cohorts: controls and disease.\n",
    "    \"\"\"\n",
    "    # Hypothetical example: lists of NIfTI paths\n",
    "    #  - Replace with your actual preprocessed fMRI NIfTI files\n",
    "    control_paths = [\"/path/to/control_subject_01_preproc.nii.gz\",\n",
    "                     \"/path/to/control_subject_02_preproc.nii.gz\",\n",
    "                     # ...\n",
    "                    ]\n",
    "    disease_paths = [\"/path/to/disease_subject_01_preproc.nii.gz\",\n",
    "                     \"/path/to/disease_subject_02_preproc.nii.gz\",\n",
    "                     # ...\n",
    "                    ]\n",
    "    \n",
    "    # 1) Load data & do subject-level PCA\n",
    "    control_data = load_fmri_data(control_paths)   # list of arrays (time x voxel)\n",
    "    disease_data = load_fmri_data(disease_paths)   # list of arrays (time x voxel)\n",
    "    \n",
    "    control_pcs = individual_subject_pca(control_data, n_components=110)\n",
    "    disease_pcs = individual_subject_pca(disease_data, n_components=110)\n",
    "    \n",
    "    # 2 & 3) Group-level PCA then ICA with repeated runs (ICASSO style)\n",
    "    ctrl_group_ics = run_group_pca_then_ica(control_pcs,\n",
    "                                            n_group_components=100,\n",
    "                                            n_ica_runs=100,\n",
    "                                            random_state=0)\n",
    "    dis_group_ics  = run_group_pca_then_ica(disease_pcs,\n",
    "                                            n_group_components=100,\n",
    "                                            n_ica_runs=100,\n",
    "                                            random_state=0)\n",
    "    # ctrl_group_ics, dis_group_ics each shape: (100, group_time_points)\n",
    "    # but for matching we usually want them as (100, voxel), i.e. \"spatial maps.\"\n",
    "    # If your group-time dimension is not the same as voxel dimension, you'd \n",
    "    # typically invert the mixing or do post-processing to obtain spatial maps.\n",
    "    # We'll pretend these are already \"spatial\" for the sake of demonstration.\n",
    "    \n",
    "    # 4) Flip negative skewness\n",
    "    ctrl_group_ics = flip_negative_skew(ctrl_group_ics)\n",
    "    dis_group_ics  = flip_negative_skew(dis_group_ics)\n",
    "    \n",
    "    # 5) Greedy matching between the two sets\n",
    "    matched_pairs, ctrl_flipped, dis_flipped = greedy_spatial_match(\n",
    "        ctrl_group_ics,\n",
    "        dis_group_ics,\n",
    "        corr_threshold=0.4\n",
    "    )\n",
    "    \n",
    "    print(f\"Number of matched IC pairs (corr>0.4): {len(matched_pairs)}\")\n",
    "    for pair in matched_pairs:\n",
    "        iA, iB, corr_val, sign_ = pair\n",
    "        print(f\"  Pair: IC_ctrl={iA}, IC_dis={iB}, corr={corr_val:.3f}, sign={sign_}\")\n",
    "    \n",
    "    # The matched_pairs with correlation > 0.4 are considered reproducible ICs. \n",
    "    # Next steps might include:\n",
    "    #   - Inspect each matched IC pair’s spatial pattern.\n",
    "    #   - Exclude artifactual components using heuristics (peak in gray matter, etc.).\n",
    "    #   - Compare final sets of reproducible ICNs across cohorts.\n",
    "    #   - Downstream connectivity/functional analyses.\n",
    "\n",
    "    print(\"Done. This demonstration performed the group-ICA-like pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42efa1-9217-4215-92a5-e3d2d72257f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5dd1af4-b87b-429c-a28e-0bf5db5bcbb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the summary spreadsheet\n",
    "spreadsheet_url = \"https://s3.amazonaws.com/fcp-indi/data/Projects/ABIDE_Initiative/Phenotypic_V1_0b.csv\"\n",
    "df = pd.read_csv(spreadsheet_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14a32130-0a41-4a20-863b-6ed15db5f894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"/blue/ruogu.fang/ryoi360/projects/fmri_vlm/results/2025_02_25_ABIDE_processing/Phenotypic_V1_0b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fe4b891-3e21-405d-8b71-68391d6d7ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>SUB_ID</th>\n",
       "      <th>FILE_ID</th>\n",
       "      <th>DX_GROUP</th>\n",
       "      <th>DSM_IV_TR</th>\n",
       "      <th>AGE_AT_SCAN</th>\n",
       "      <th>SEX</th>\n",
       "      <th>HANDEDNESS_CATEGORY</th>\n",
       "      <th>HANDEDNESS_SCORES</th>\n",
       "      <th>FIQ</th>\n",
       "      <th>...</th>\n",
       "      <th>WISC_IV_BLK_DSN_SCALED</th>\n",
       "      <th>WISC_IV_PIC_CON_SCALED</th>\n",
       "      <th>WISC_IV_MATRIX_SCALED</th>\n",
       "      <th>WISC_IV_DIGIT_SPAN_SCALED</th>\n",
       "      <th>WISC_IV_LET_NUM_SCALED</th>\n",
       "      <th>WISC_IV_CODING_SCALED</th>\n",
       "      <th>WISC_IV_SYM_SCALED</th>\n",
       "      <th>EYE_STATUS_AT_SCAN</th>\n",
       "      <th>AGE_AT_MPRAGE</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CALTECH</td>\n",
       "      <td>51456</td>\n",
       "      <td>Caltech_0051456</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>55.40</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CALTECH</td>\n",
       "      <td>51457</td>\n",
       "      <td>Caltech_0051457</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>22.90</td>\n",
       "      <td>1</td>\n",
       "      <td>Ambi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CALTECH</td>\n",
       "      <td>51458</td>\n",
       "      <td>Caltech_0051458</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39.20</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CALTECH</td>\n",
       "      <td>51459</td>\n",
       "      <td>Caltech_0051459</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.80</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CALTECH</td>\n",
       "      <td>51460</td>\n",
       "      <td>Caltech_0051460</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2</td>\n",
       "      <td>Ambi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>YALE</td>\n",
       "      <td>50624</td>\n",
       "      <td>Yale_0050624</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11.08</td>\n",
       "      <td>2</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>YALE</td>\n",
       "      <td>50625</td>\n",
       "      <td>Yale_0050625</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>YALE</td>\n",
       "      <td>50626</td>\n",
       "      <td>Yale_0050626</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11.08</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>YALE</td>\n",
       "      <td>50627</td>\n",
       "      <td>Yale_0050627</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9.50</td>\n",
       "      <td>2</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>YALE</td>\n",
       "      <td>50628</td>\n",
       "      <td>Yale_0050628</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14.42</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1112 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SITE_ID  SUB_ID          FILE_ID  DX_GROUP  DSM_IV_TR  AGE_AT_SCAN  SEX  \\\n",
       "0     CALTECH   51456  Caltech_0051456         1          4        55.40    1   \n",
       "1     CALTECH   51457  Caltech_0051457         1          4        22.90    1   \n",
       "2     CALTECH   51458  Caltech_0051458         1          1        39.20    1   \n",
       "3     CALTECH   51459  Caltech_0051459         1          1        22.80    1   \n",
       "4     CALTECH   51460  Caltech_0051460         1          1        34.60    2   \n",
       "...       ...     ...              ...       ...        ...          ...  ...   \n",
       "1107     YALE   50624     Yale_0050624         1          3        11.08    2   \n",
       "1108     YALE   50625     Yale_0050625         1          3         7.00    1   \n",
       "1109     YALE   50626     Yale_0050626         1          3        11.08    1   \n",
       "1110     YALE   50627     Yale_0050627         1          3         9.50    2   \n",
       "1111     YALE   50628     Yale_0050628         1          3        14.42    1   \n",
       "\n",
       "     HANDEDNESS_CATEGORY  HANDEDNESS_SCORES    FIQ  ...  \\\n",
       "0                      R                NaN  126.0  ...   \n",
       "1                   Ambi                NaN  107.0  ...   \n",
       "2                      R                NaN   93.0  ...   \n",
       "3                      R                NaN  106.0  ...   \n",
       "4                   Ambi                NaN  133.0  ...   \n",
       "...                  ...                ...    ...  ...   \n",
       "1107                   R                NaN   90.0  ...   \n",
       "1108                   L                NaN   99.0  ...   \n",
       "1109                   L                NaN   61.0  ...   \n",
       "1110                   R                NaN   88.0  ...   \n",
       "1111                   R                NaN   77.0  ...   \n",
       "\n",
       "      WISC_IV_BLK_DSN_SCALED  WISC_IV_PIC_CON_SCALED WISC_IV_MATRIX_SCALED  \\\n",
       "0                        NaN                     NaN                   NaN   \n",
       "1                        NaN                     NaN                   NaN   \n",
       "2                        NaN                     NaN                   NaN   \n",
       "3                        NaN                     NaN                   NaN   \n",
       "4                        NaN                     NaN                   NaN   \n",
       "...                      ...                     ...                   ...   \n",
       "1107                     NaN                     NaN                   NaN   \n",
       "1108                     NaN                     NaN                   NaN   \n",
       "1109                     NaN                     NaN                   NaN   \n",
       "1110                     NaN                     NaN                   NaN   \n",
       "1111                     NaN                     NaN                   NaN   \n",
       "\n",
       "     WISC_IV_DIGIT_SPAN_SCALED WISC_IV_LET_NUM_SCALED  WISC_IV_CODING_SCALED  \\\n",
       "0                          NaN                    NaN                    NaN   \n",
       "1                          NaN                    NaN                    NaN   \n",
       "2                          NaN                    NaN                    NaN   \n",
       "3                          NaN                    NaN                    NaN   \n",
       "4                          NaN                    NaN                    NaN   \n",
       "...                        ...                    ...                    ...   \n",
       "1107                       NaN                    NaN                    NaN   \n",
       "1108                       NaN                    NaN                    NaN   \n",
       "1109                       NaN                    NaN                    NaN   \n",
       "1110                       NaN                    NaN                    NaN   \n",
       "1111                       NaN                    NaN                    NaN   \n",
       "\n",
       "      WISC_IV_SYM_SCALED  EYE_STATUS_AT_SCAN  AGE_AT_MPRAGE  BMI  \n",
       "0                    NaN                   2            NaN  NaN  \n",
       "1                    NaN                   2            NaN  NaN  \n",
       "2                    NaN                   2            NaN  NaN  \n",
       "3                    NaN                   2            NaN  NaN  \n",
       "4                    NaN                   2            NaN  NaN  \n",
       "...                  ...                 ...            ...  ...  \n",
       "1107                 NaN                   1            NaN  NaN  \n",
       "1108                 NaN                   1            NaN  NaN  \n",
       "1109                 NaN                   1            NaN  NaN  \n",
       "1110                 NaN                   1            NaN  NaN  \n",
       "1111                 NaN                   1            NaN  NaN  \n",
       "\n",
       "[1112 rows x 75 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c13bc8-0373-4f7b-bcaf-b52b2616e0a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1112"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"SUB_ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "521d34b7-fda8-4db2-b428-d1ad5fd635c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CALTECH', 'CMU', 'KKI', 'LEUVEN_1', 'LEUVEN_2', 'MAX_MUN', 'NYU',\n",
       "       'OHSU', 'OLIN', 'PITT', 'SBL', 'SDSU', 'STANFORD', 'TRINITY',\n",
       "       'UCLA_1', 'UCLA_2', 'UM_1', 'UM_2', 'USM', 'YALE'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"SITE_ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f42e96cf-4df9-4fdf-a99e-c5d867b5f333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"DX_GROUP\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbcfdab0-a239-40bd-8c8e-09d7532ba1c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SITE_ID', 'SUB_ID', 'FILE_ID', 'DX_GROUP', 'DSM_IV_TR', 'AGE_AT_SCAN',\n",
       "       'SEX', 'HANDEDNESS_CATEGORY', 'HANDEDNESS_SCORES', 'FIQ', 'VIQ', 'PIQ',\n",
       "       'FIQ_TEST_TYPE', 'VIQ_TEST_TYPE', 'PIQ_TEST_TYPE',\n",
       "       'ADI_R_SOCIAL_TOTAL_A', 'ADI_R_VERBAL_TOTAL_BV', 'ADI_RRB_TOTAL_C',\n",
       "       'ADI_R_ONSET_TOTAL_D', 'ADI_R_RSRCH_RELIABLE', 'ADOS_MODULE',\n",
       "       'ADOS_TOTAL', 'ADOS_COMM', 'ADOS_SOCIAL', 'ADOS_STEREO_BEHAV',\n",
       "       'ADOS_RSRCH_RELIABLE', 'ADOS_GOTHAM_SOCAFFECT', 'ADOS_GOTHAM_RRB',\n",
       "       'ADOS_GOTHAM_TOTAL', 'ADOS_GOTHAM_SEVERITY', 'SRS_VERSION',\n",
       "       'SRS_RAW_TOTAL', 'SRS_AWARENESS', 'SRS_COGNITION', 'SRS_COMMUNICATION',\n",
       "       'SRS_MOTIVATION', 'SRS_MANNERISMS', 'SCQ_TOTAL', 'AQ_TOTAL',\n",
       "       'COMORBIDITY', 'CURRENT_MED_STATUS', 'MEDICATION_NAME',\n",
       "       'OFF_STIMULANTS_AT_SCAN', 'VINELAND_RECEPTIVE_V_SCALED',\n",
       "       'VINELAND_EXPRESSIVE_V_SCALED', 'VINELAND_WRITTEN_V_SCALED',\n",
       "       'VINELAND_COMMUNICATION_STANDARD', 'VINELAND_PERSONAL_V_SCALED',\n",
       "       'VINELAND_DOMESTIC_V_SCALED', 'VINELAND_COMMUNITY_V_SCALED',\n",
       "       'VINELAND_DAILYLVNG_STANDARD', 'VINELAND_INTERPERSONAL_V_SCALED',\n",
       "       'VINELAND_PLAY_V_SCALED', 'VINELAND_COPING_V_SCALED',\n",
       "       'VINELAND_SOCIAL_STANDARD', 'VINELAND_SUM_SCORES',\n",
       "       'VINELAND_ABC_STANDARD', 'VINELAND_INFORMANT', 'WISC_IV_VCI',\n",
       "       'WISC_IV_PRI', 'WISC_IV_WMI', 'WISC_IV_PSI', 'WISC_IV_SIM_SCALED',\n",
       "       'WISC_IV_VOCAB_SCALED', 'WISC_IV_INFO_SCALED', 'WISC_IV_BLK_DSN_SCALED',\n",
       "       'WISC_IV_PIC_CON_SCALED', 'WISC_IV_MATRIX_SCALED',\n",
       "       'WISC_IV_DIGIT_SPAN_SCALED', 'WISC_IV_LET_NUM_SCALED',\n",
       "       'WISC_IV_CODING_SCALED', 'WISC_IV_SYM_SCALED', 'EYE_STATUS_AT_SCAN',\n",
       "       'AGE_AT_MPRAGE', 'BMI'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d35837f6-a163-448a-859b-e3090da79e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_metadata_df = pd.read_csv(\"/blue/ruogu.fang/ryoi360/projects/fmri_vlm/results/2025_02_25_ABIDE_processing/Phenotypic_V1_0b_preprocessed1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f0b8635-ae09-47e9-93f6-3e4ec655cd66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0.1\n",
      "Unnamed: 0\n",
      "SUB_ID\n",
      "X\n",
      "subject\n",
      "SITE_ID\n",
      "FILE_ID\n",
      "DX_GROUP\n",
      "DSM_IV_TR\n",
      "AGE_AT_SCAN\n",
      "SEX\n",
      "HANDEDNESS_CATEGORY\n",
      "HANDEDNESS_SCORES\n",
      "FIQ\n",
      "VIQ\n",
      "PIQ\n",
      "FIQ_TEST_TYPE\n",
      "VIQ_TEST_TYPE\n",
      "PIQ_TEST_TYPE\n",
      "ADI_R_SOCIAL_TOTAL_A\n",
      "ADI_R_VERBAL_TOTAL_BV\n",
      "ADI_RRB_TOTAL_C\n",
      "ADI_R_ONSET_TOTAL_D\n",
      "ADI_R_RSRCH_RELIABLE\n",
      "ADOS_MODULE\n",
      "ADOS_TOTAL\n",
      "ADOS_COMM\n",
      "ADOS_SOCIAL\n",
      "ADOS_STEREO_BEHAV\n",
      "ADOS_RSRCH_RELIABLE\n",
      "ADOS_GOTHAM_SOCAFFECT\n",
      "ADOS_GOTHAM_RRB\n",
      "ADOS_GOTHAM_TOTAL\n",
      "ADOS_GOTHAM_SEVERITY\n",
      "SRS_VERSION\n",
      "SRS_RAW_TOTAL\n",
      "SRS_AWARENESS\n",
      "SRS_COGNITION\n",
      "SRS_COMMUNICATION\n",
      "SRS_MOTIVATION\n",
      "SRS_MANNERISMS\n",
      "SCQ_TOTAL\n",
      "AQ_TOTAL\n",
      "COMORBIDITY\n",
      "CURRENT_MED_STATUS\n",
      "MEDICATION_NAME\n",
      "OFF_STIMULANTS_AT_SCAN\n",
      "VINELAND_RECEPTIVE_V_SCALED\n",
      "VINELAND_EXPRESSIVE_V_SCALED\n",
      "VINELAND_WRITTEN_V_SCALED\n",
      "VINELAND_COMMUNICATION_STANDARD\n",
      "VINELAND_PERSONAL_V_SCALED\n",
      "VINELAND_DOMESTIC_V_SCALED\n",
      "VINELAND_COMMUNITY_V_SCALED\n",
      "VINELAND_DAILYLVNG_STANDARD\n",
      "VINELAND_INTERPERSONAL_V_SCALED\n",
      "VINELAND_PLAY_V_SCALED\n",
      "VINELAND_COPING_V_SCALED\n",
      "VINELAND_SOCIAL_STANDARD\n",
      "VINELAND_SUM_SCORES\n",
      "VINELAND_ABC_STANDARD\n",
      "VINELAND_INFORMANT\n",
      "WISC_IV_VCI\n",
      "WISC_IV_PRI\n",
      "WISC_IV_WMI\n",
      "WISC_IV_PSI\n",
      "WISC_IV_SIM_SCALED\n",
      "WISC_IV_VOCAB_SCALED\n",
      "WISC_IV_INFO_SCALED\n",
      "WISC_IV_BLK_DSN_SCALED\n",
      "WISC_IV_PIC_CON_SCALED\n",
      "WISC_IV_MATRIX_SCALED\n",
      "WISC_IV_DIGIT_SPAN_SCALED\n",
      "WISC_IV_LET_NUM_SCALED\n",
      "WISC_IV_CODING_SCALED\n",
      "WISC_IV_SYM_SCALED\n",
      "EYE_STATUS_AT_SCAN\n",
      "AGE_AT_MPRAGE\n",
      "BMI\n",
      "anat_cnr\n",
      "anat_efc\n",
      "anat_fber\n",
      "anat_fwhm\n",
      "anat_qi1\n",
      "anat_snr\n",
      "func_efc\n",
      "func_fber\n",
      "func_fwhm\n",
      "func_dvars\n",
      "func_outlier\n",
      "func_quality\n",
      "func_mean_fd\n",
      "func_num_fd\n",
      "func_perc_fd\n",
      "func_gsr\n",
      "qc_rater_1\n",
      "qc_notes_rater_1\n",
      "qc_anat_rater_2\n",
      "qc_anat_notes_rater_2\n",
      "qc_func_rater_2\n",
      "qc_func_notes_rater_2\n",
      "qc_anat_rater_3\n",
      "qc_anat_notes_rater_3\n",
      "qc_func_rater_3\n",
      "qc_func_notes_rater_3\n",
      "SUB_IN_SMP\n"
     ]
    }
   ],
   "source": [
    "for col in preprocessed_metadata_df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e05132c9-ef3c-4999-942a-fe05f7a9e5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects after stricter filtering: 714\n"
     ]
    }
   ],
   "source": [
    "motion_filtered_df = preprocessed_metadata_df[(preprocessed_metadata_df['func_mean_fd'] <= 0.2) & (preprocessed_metadata_df['func_num_fd'] < 20)]\n",
    "\n",
    "print(f\"Subjects after stricter filtering: {len(motion_filtered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "290e81cb-20b4-4edc-9f8c-10a9f8ca6721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_metadata_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ebd4d70-7982-47f6-9b04-5da9f56575fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.116828\n",
       "1       0.322092\n",
       "2       0.127745\n",
       "3       0.128136\n",
       "4       0.070143\n",
       "          ...   \n",
       "1107    0.116186\n",
       "1108    0.140171\n",
       "1109    0.154887\n",
       "1110    0.048246\n",
       "1111    0.168913\n",
       "Name: func_mean_fd, Length: 1112, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_metadata_df[\"func_mean_fd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d39ebbd9-293f-47e7-9167-4678fe35aa7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.7534808758)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_metadata_df[\"func_fwhm\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20545cfc-fb47-4a33-a801-fe4664dd3d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5667c5b9-d64f-4c91-b38b-6b0840dde23d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abide_files = glob.glob(\"/orange/ruogu.fang/ryoi360/ABIDE/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de8274fc-96af-414d-862c-2d610b4050db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1035"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abide_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a7892f4-7dee-4a8a-b994-3791df45cb1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20049441-258c-4119-8e43-9a12f042236f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fmri_img = nib.load(abide_files[0])\n",
    "fmri_data = fmri_img.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f3182e2-e867-47fa-866a-709987278cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 73, 61, 116)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmri_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "940701ed-44a5-40b4-b02e-22ee8ac70953",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Voxel Size (mm): [3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Get the voxel size from the affine transformation matrix\n",
    "voxel_size = np.sqrt(np.sum(fmri_img.affine[:3, :3] ** 2, axis=0))\n",
    "print(\"Original Voxel Size (mm):\", voxel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b288a84-e956-413e-889d-f16ed92027fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Install Required Packages\n",
    "If you haven't installed Nipype, Nibabel, and NiLearn, do so using:\n",
    "\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pip install nipype nibabel nilearn numpy scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58581f4-646d-4b58-a50b-6a004fa5679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Preprocessing Steps in Python\n",
    "Step 1: Rigid Body Motion Correction\n",
    "Use SPM's Realign function via Nipype.\n",
    "Alternatively, use FSL's MCFLIRT.\n",
    "SPM12 (via Nipype)\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from nipype.interfaces.spm import Realign\n",
    "\n",
    "realign = Realign()\n",
    "realign.inputs.in_files = 'subject_func.nii'  # Replace with your file path\n",
    "realign.inputs.register_to_mean = True\n",
    "realign.run()\n",
    "FSL Alternative\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from nipype.interfaces.fsl import MCFLIRT\n",
    "\n",
    "mcflirt = MCFLIRT()\n",
    "mcflirt.inputs.in_file = 'subject_func.nii'\n",
    "mcflirt.inputs.out_file = 'motion_corrected.nii'\n",
    "mcflirt.run()\n",
    "Step 2: Slice Timing Correction\n",
    "Adjusts for differences in slice acquisition time.\n",
    "Requires TR (repetition time) and slice order.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from nipype.interfaces.spm import SliceTiming\n",
    "\n",
    "slice_timing = SliceTiming()\n",
    "slice_timing.inputs.in_files = 'motion_corrected.nii'\n",
    "slice_timing.inputs.time_repetition = 2.0  # Set the correct TR\n",
    "slice_timing.run()\n",
    "Step 3: Normalization to MNI Space\n",
    "Warp the functional data into MNI152 template.\n",
    "Use SPM's Normalize or FSL's FLIRT/FNIRT.\n",
    "SPM Normalization\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from nipype.interfaces.spm import Normalize12\n",
    "\n",
    "normalize = Normalize12()\n",
    "normalize.inputs.image_to_align = 'slice_time_corrected.nii'\n",
    "normalize.inputs.apply_to_files = ['slice_time_corrected.nii']\n",
    "normalize.inputs.jobtype = 'estwrite'  # Estimate and apply transformation\n",
    "normalize.run()\n",
    "FSL FLIRT Alternative\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from nipype.interfaces.fsl import FLIRT\n",
    "\n",
    "flirt = FLIRT()\n",
    "flirt.inputs.in_file = 'slice_time_corrected.nii'\n",
    "flirt.inputs.reference = '/usr/local/fsl/data/standard/MNI152_T1_2mm_brain.nii.gz'\n",
    "flirt.inputs.out_file = 'normalized.nii'\n",
    "flirt.run()\n",
    "Step 4: Resampling to 3×3×3 mm³\n",
    "Use NiLearn for resampling.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from nilearn.image import resample_img\n",
    "import nibabel as nib\n",
    "\n",
    "img = nib.load(\"normalized.nii\")\n",
    "\n",
    "resampled_img = resample_img(img, target_affine=np.diag([3, 3, 3, 1]))\n",
    "nib.save(resampled_img, \"resampled_3mm.nii\")\n",
    "Step 5: Spatial Smoothing (FWHM = 6 mm)\n",
    "Apply Gaussian smoothing using NiLearn.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from nilearn.image import smooth_img\n",
    "\n",
    "smoothed_img = smooth_img(\"resampled_3mm.nii\", fwhm=6)\n",
    "smoothed_img.to_filename(\"smoothed.nii\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14084270-635f-420d-bc8e-092d69ec081e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607fa7e-49be-4e90-8af1-662dcc87ef38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda79c1-0261-40ec-a0de-e14f8945a2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68818b9c-bbed-41bc-9bf2-b310731897be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nilabel_env",
   "language": "python",
   "name": "nibabel_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
